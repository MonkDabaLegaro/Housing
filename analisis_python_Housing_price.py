# en la terminal antes de ejecutar este script, instalar las librer√≠as necesarias con:
# pip install pandas numpy matplotlib seaborn scipy scikit-learn missingno

from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import label_binarize
from sklearn.feature_selection import RFE
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc
from sklearn.decomposition import PCA
import missingno as msno
import warnings
from sklearn.metrics import precision_score, recall_score, f1_score
warnings.filterwarnings('ignore')

# Configurar estilo de gr√°ficos
plt.style.use('default')
sns.set_palette("husl")

print("="*80)
print("AN√ÅLISIS EXPLORATORIO DE DATOS - PRECIOS DE VIVIENDAS")
print("="*80)

# Cargar datos
df = pd.read_csv('Housing_price_prediction.csv')

# 1. Descripci√≥n del conjunto de datos
print("\n1. DESCRIPCI√ìN DEL CONJUNTO DE DATOS:")
print("-" * 50)
print(f"üìä Dimensiones del dataset: {df.shape[0]} observaciones (filas) √ó {df.shape[1]} variables (columnas)")
print(f"üìà Memoria utilizada: {df.memory_usage(deep=True).sum() / 1024:.2f} KB")

print("\nüìã TIPOS DE VARIABLES:")
for col, dtype in df.dtypes.items():
    unique_vals = df[col].nunique()
    print(f"  ‚Ä¢ {col:20} ‚Üí {str(dtype):10} ({unique_vals:3} valores √∫nicos)")

print("\nüîç VISTA PREVIA DE LOS DATOS:")
print(df.head().to_string())

print("\nüìä INFORMACI√ìN GENERAL:")
print(f"  ‚Ä¢ Variables categ√≥ricas: {len(df.select_dtypes(include=['object']).columns)}")
print(f"  ‚Ä¢ Variables num√©ricas: {len(df.select_dtypes(include=[np.number]).columns)}")
print(f"  ‚Ä¢ Valores faltantes totales: {df.isnull().sum().sum()}")

# 2. An√°lisis gr√°fico de variables categ√≥ricas (MEJORADO)
print("\n" + "="*80)
print("2. AN√ÅLISIS DE VARIABLES CATEG√ìRICAS")
print("="*80)

categorical_cols = ['mainroad', 'guestroom', 'basement',
                    'hotwaterheating', 'airconditioning', 'prefarea', 'furnishingstatus']

# Crear tabla de frecuencias para cada variable categ√≥rica
print("\nüìä TABLA DE FRECUENCIAS:")
for col in categorical_cols:
    print(f"\n{col.upper()}:")
    freq_table = df[col].value_counts()
    perc_table = df[col].value_counts(normalize=True) * 100
    combined = pd.DataFrame({
        'Frecuencia': freq_table,
        'Porcentaje': perc_table.round(2)
    })
    print(combined.to_string())

# Gr√°fico mejorado con t√≠tulos y etiquetas claras
fig, axes = plt.subplots(3, 3, figsize=(18, 12))
fig.suptitle('Distribuci√≥n de Variables Categ√≥ricas en el Dataset de Viviendas', 
             fontsize=16, fontweight='bold', y=0.98)

for i, col in enumerate(categorical_cols):
    row = i // 3
    col_idx = i % 3
    ax = axes[row, col_idx]
    
    counts = df[col].value_counts()
    bars = ax.bar(counts.index, counts.values, alpha=0.8, edgecolor='black', linewidth=0.5)
    
    # A√±adir valores en las barras
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 5,
                f'{int(height)}', ha='center', va='bottom', fontweight='bold')
    
    ax.set_title(f'{col.replace("_", " ").title()}', fontsize=12, fontweight='bold', pad=10)
    ax.set_xlabel('Categor√≠as', fontsize=10, fontweight='bold')
    ax.set_ylabel('N√∫mero de Viviendas', fontsize=10, fontweight='bold')
    ax.grid(axis='y', alpha=0.3)
    
    # Rotar etiquetas si son largas
    if len(str(counts.index[0])) > 5:
        ax.tick_params(axis='x', rotation=45)

# Eliminar subplot vac√≠o
axes[2, 2].remove()

plt.tight_layout()
plt.subplots_adjust(top=0.93)
plt.show()

# 3. Tabla resumen de variables num√©ricas (MEJORADA)
print("\n" + "="*80)
print("3. AN√ÅLISIS ESTAD√çSTICO DE VARIABLES NUM√âRICAS")
print("="*80)

numeric_cols = ['price', 'area', 'bedrooms', 'bathrooms', 'stories', 'parking']

print("\nüìä MEDIDAS ESTAD√çSTICAS DESCRIPTIVAS:")
print("F√≥rmulas aplicadas:")
print("  ‚Ä¢ Media (Œº) = Œ£xi/n")
print("  ‚Ä¢ Desviaci√≥n est√°ndar (œÉ) = ‚àö(Œ£(xi-Œº)¬≤/(n-1))")
print("  ‚Ä¢ Percentiles: valores que dividen los datos ordenados")
print("  ‚Ä¢ Rango intercuart√≠lico (IQR) = Q3 - Q1")

desc_stats = df[numeric_cols].describe()
print("\n" + desc_stats.to_string())

# Informaci√≥n adicional
print("\nüìà INFORMACI√ìN ADICIONAL:")
for col in numeric_cols:
    skewness = df[col].skew()
    kurtosis = df[col].kurtosis()
    print(f"\n{col.upper()}:")
    print(f"  ‚Ä¢ Asimetr√≠a (Skewness): {skewness:.3f} {'(Sesgada a la derecha)' if skewness > 0 else '(Sesgada a la izquierda)' if skewness < 0 else '(Sim√©trica)'}")
    print(f"  ‚Ä¢ Curtosis: {kurtosis:.3f} {'(Leptoc√∫rtica - m√°s puntiaguda)' if kurtosis > 0 else '(Platic√∫rtica - m√°s plana)' if kurtosis < 0 else '(Mesoc√∫rtica - normal)'}")
    print(f"  ‚Ä¢ Rango: {df[col].min():.0f} - {df[col].max():.0f}")
    print(f"  ‚Ä¢ Coeficiente de variaci√≥n: {(df[col].std()/df[col].mean()*100):.2f}%")

# 4. Histograma con curva de densidad para 'price' (MEJORADO)
print("\n" + "="*80)
print("4. AN√ÅLISIS DE DISTRIBUCI√ìN DE PRECIOS")
print("="*80)

print("\nüìä ESTAD√çSTICAS DETALLADAS DE PRECIOS:")
price_stats = {
    'Media': df['price'].mean(),
    'Mediana': df['price'].median(),
    'Moda': df['price'].mode().iloc[0],
    'Desviaci√≥n Est√°ndar': df['price'].std(),
    'Varianza': df['price'].var(),
    'Rango': df['price'].max() - df['price'].min(),
    'IQR': df['price'].quantile(0.75) - df['price'].quantile(0.25)
}

for stat, value in price_stats.items():
    print(f"  ‚Ä¢ {stat}: ${value:,.2f}")

# Crear bins m√°s informativos
n_bins = 25
plt.figure(figsize=(14, 8))

# Histograma con m√°s detalle
n, bins, patches = plt.hist(df['price'], bins=n_bins, alpha=0.7, color='skyblue', 
                           edgecolor='black', linewidth=0.5, density=True)

# Curva de densidad
kde_x = np.linspace(df['price'].min(), df['price'].max(), 100)
kde = stats.gaussian_kde(df['price'])
plt.plot(kde_x, kde(kde_x), 'r-', linewidth=2, label='Curva de Densidad (KDE)')

# L√≠neas de referencia
plt.axvline(df['price'].mean(), color='red', linestyle='--', linewidth=2, 
           label=f'Media: ${df["price"].mean():,.0f}')
plt.axvline(df['price'].median(), color='green', linestyle='--', linewidth=2, 
           label=f'Mediana: ${df["price"].median():,.0f}')

plt.title('Distribuci√≥n de Precios de Viviendas\n(Histograma con Curva de Densidad)', 
          fontsize=14, fontweight='bold', pad=20)
plt.xlabel('Precio de la Vivienda (USD)', fontsize=12, fontweight='bold')
plt.ylabel('Densidad de Probabilidad', fontsize=12, fontweight='bold')
plt.legend(fontsize=10)
plt.grid(axis='y', alpha=0.3)

# Formato de n√∫meros en el eje x
ax = plt.gca()
ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))

plt.tight_layout()
plt.show()

print(f"\nüìà INTERPRETACI√ìN DE LA DISTRIBUCI√ìN:")
skew_price = df['price'].skew()
if skew_price > 0.5:
    print("  ‚Ä¢ La distribuci√≥n est√° SESGADA A LA DERECHA (cola larga hacia valores altos)")
    print("  ‚Ä¢ Esto indica que hay m√°s viviendas de precio bajo-medio y pocas muy caras")
elif skew_price < -0.5:
    print("  ‚Ä¢ La distribuci√≥n est√° SESGADA A LA IZQUIERDA (cola larga hacia valores bajos)")
else:
    print("  ‚Ä¢ La distribuci√≥n es aproximadamente SIM√âTRICA")

# 5. An√°lisis de normalidad para 'price' (Q-Q plot y Shapiro-Wilk) - MEJORADO
print("\n" + "="*80)
print("5. AN√ÅLISIS DE NORMALIDAD - GR√ÅFICO Q-Q")
print("="*80)

print("\nüîç ¬øQU√â ES UN GR√ÅFICO Q-Q?")
print("El gr√°fico Q-Q (Quantile-Quantile) compara los cuantiles de nuestros datos")
print("con los cuantiles de una distribuci√≥n normal te√≥rica.")
print("\nF√≥rmula matem√°tica:")
print("  ‚Ä¢ Cuantil te√≥rico: Œ¶‚Åª¬π(p) donde Œ¶ es la funci√≥n de distribuci√≥n normal")
print("  ‚Ä¢ Cuantil observado: valor en la posici√≥n p de los datos ordenados")
print("\nInterpretaci√≥n:")
print("  ‚Ä¢ Si los puntos siguen la l√≠nea diagonal ‚Üí datos normales")
print("  ‚Ä¢ Curvatura hacia arriba ‚Üí cola derecha m√°s pesada")
print("  ‚Ä¢ Curvatura hacia abajo ‚Üí cola izquierda m√°s pesada")

plt.figure(figsize=(12, 6))

# Q-Q plot
plt.subplot(1, 2, 1)
stats.probplot(df['price'], dist="norm", plot=plt)
plt.title('Gr√°fico Q-Q: Precios vs Distribuci√≥n Normal\n(An√°lisis de Normalidad)', 
          fontsize=12, fontweight='bold')
plt.xlabel('Cuantiles Te√≥ricos (Distribuci√≥n Normal)', fontsize=10, fontweight='bold')
plt.ylabel('Cuantiles Observados (Precios)', fontsize=10, fontweight='bold')
plt.grid(alpha=0.3)

# Histograma comparativo
plt.subplot(1, 2, 2)
plt.hist(df['price'], bins=30, density=True, alpha=0.7, color='lightblue', 
         edgecolor='black', label='Datos Observados')

# Distribuci√≥n normal te√≥rica
mu, sigma = df['price'].mean(), df['price'].std()
x = np.linspace(df['price'].min(), df['price'].max(), 100)
normal_dist = stats.norm.pdf(x, mu, sigma)
plt.plot(x, normal_dist, 'r-', linewidth=2, label='Distribuci√≥n Normal Te√≥rica')

plt.title('Comparaci√≥n: Datos vs Distribuci√≥n Normal', fontsize=12, fontweight='bold')
plt.xlabel('Precio de la Vivienda', fontsize=10, fontweight='bold')
plt.ylabel('Densidad', fontsize=10, fontweight='bold')
plt.legend()
plt.grid(alpha=0.3)

plt.tight_layout()
plt.show()

# Test de Shapiro-Wilk
stat, p_value = stats.shapiro(df['price'])
print(f"\nüìä TEST DE SHAPIRO-WILK:")
print(f"F√≥rmula: W = (Œ£a·µ¢x‚Çç·µ¢‚Çé)¬≤ / Œ£(x·µ¢ - xÃÑ)¬≤")
print(f"  ‚Ä¢ Estad√≠stico W: {stat:.6f}")
print(f"  ‚Ä¢ Valor p: {p_value:.2e}")
print(f"  ‚Ä¢ Interpretaci√≥n: {'Los datos NO siguen distribuci√≥n normal' if p_value < 0.05 else 'Los datos siguen distribuci√≥n normal'} (Œ± = 0.05)")

print(f"\nüéØ CONCLUSI√ìN SOBRE NORMALIDAD:")
if p_value < 0.05:
    print("  ‚Ä¢ Los precios NO siguen una distribuci√≥n normal")
    print("  ‚Ä¢ Esto es L√ìGICO porque los precios de viviendas t√≠picamente:")
    print("    - Tienen un l√≠mite inferior (no pueden ser negativos)")
    print("    - Presentan sesgo hacia la derecha (pocas casas muy caras)")
    print("    - Reflejan la realidad del mercado inmobiliario")
else:
    print("  ‚Ä¢ Los precios siguen aproximadamente una distribuci√≥n normal")

# 6. Identificaci√≥n de datos at√≠picos (boxplot) - MEJORADO
print("\n" + "="*80)
print("6. AN√ÅLISIS DE DATOS AT√çPICOS (OUTLIERS)")
print("="*80)

print("\nüìä F√ìRMULAS PARA DETECCI√ìN DE OUTLIERS:")
print("M√©todo del Rango Intercuart√≠lico (IQR):")
print("  ‚Ä¢ Q1 = Percentil 25")
print("  ‚Ä¢ Q3 = Percentil 75") 
print("  ‚Ä¢ IQR = Q3 - Q1")
print("  ‚Ä¢ L√≠mite inferior = Q1 - 1.5 √ó IQR")
print("  ‚Ä¢ L√≠mite superior = Q3 + 1.5 √ó IQR")
print("  ‚Ä¢ Outlier: valor < l√≠mite inferior O valor > l√≠mite superior")

# Calcular outliers para cada variable
outlier_info = {}
for col in numeric_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]
    
    outlier_info[col] = {
        'Q1': Q1, 'Q3': Q3, 'IQR': IQR,
        'lower_bound': lower_bound, 'upper_bound': upper_bound,
        'n_outliers': len(outliers), 'outliers': outliers.tolist()
    }

print(f"\nüìà RESUMEN DE OUTLIERS POR VARIABLE:")
for col, info in outlier_info.items():
    print(f"\n{col.upper()}:")
    print(f"  ‚Ä¢ Rango normal: [{info['lower_bound']:.2f}, {info['upper_bound']:.2f}]")
    print(f"  ‚Ä¢ N√∫mero de outliers: {info['n_outliers']} ({info['n_outliers']/len(df)*100:.1f}% del total)")
    if info['n_outliers'] > 0 and info['n_outliers'] <= 10:
        print(f"  ‚Ä¢ Valores at√≠picos: {info['outliers']}")

# Boxplot mejorado
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
fig.suptitle('Diagramas de Caja para Variables Num√©ricas\n(Detecci√≥n de Valores At√≠picos)', 
             fontsize=16, fontweight='bold', y=0.98)

for i, col in enumerate(numeric_cols):
    row = i // 3
    col_idx = i % 3
    ax = axes[row, col_idx]
    
    # Crear boxplot
    bp = ax.boxplot(df[col], patch_artist=True, notch=True)
    bp['boxes'][0].set_facecolor('lightblue')
    bp['boxes'][0].set_alpha(0.7)
    
    # A√±adir estad√≠sticas
    stats_text = f"Mediana: {df[col].median():.1f}\nIQR: {outlier_info[col]['IQR']:.1f}\nOutliers: {outlier_info[col]['n_outliers']}"
    ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=9,
            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
    
    ax.set_title(f'{col.replace("_", " ").title()}', fontsize=12, fontweight='bold')
    ax.set_ylabel(f'Valores de {col.title()}', fontsize=10, fontweight='bold')
    ax.set_xlabel('Variable', fontsize=10, fontweight='bold')
    ax.grid(axis='y', alpha=0.3)
    
    # Formato especial para precio
    if col == 'price':
        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))

plt.tight_layout()
plt.subplots_adjust(top=0.93)
plt.show()

print(f"\nüéØ INTERPRETACI√ìN DE LOS BOXPLOTS:")
print("  ‚Ä¢ La 'caja' representa el 50% central de los datos (Q1 a Q3)")
print("  ‚Ä¢ La l√≠nea dentro de la caja es la mediana")
print("  ‚Ä¢ Los 'bigotes' se extienden hasta 1.5√óIQR desde los cuartiles")
print("  ‚Ä¢ Los puntos fuera de los bigotes son outliers")
print("  ‚Ä¢ Es NORMAL que 'price' tenga la escala m√°s amplia (valores en miles)")

# 7. Relaci√≥n entre variable num√©rica y categ√≥rica (MEJORADO)
print("\n" + "="*80)
print("7. AN√ÅLISIS: PRECIO POR ESTADO DE AMUEBLADO")
print("="*80)

print("\nüìä ESTAD√çSTICAS POR CATEGOR√çA:")
price_by_furnishing = df.groupby('furnishingstatus')['price'].agg([
    'count', 'mean', 'median', 'std', 'min', 'max'
]).round(2)
print(price_by_furnishing.to_string())

# Test ANOVA para diferencias significativas
groups = [group['price'].values for name, group in df.groupby('furnishingstatus')]
f_stat, p_val_anova = stats.f_oneway(*groups)

print(f"\nüìà TEST ANOVA (An√°lisis de Varianza):")
print(f"F√≥rmula: F = (Varianza entre grupos) / (Varianza dentro de grupos)")
print(f"  ‚Ä¢ Estad√≠stico F: {f_stat:.4f}")
print(f"  ‚Ä¢ Valor p: {p_val_anova:.6f}")
print(f"  ‚Ä¢ Interpretaci√≥n: {'Hay diferencias significativas' if p_val_anova < 0.05 else 'No hay diferencias significativas'} entre grupos (Œ± = 0.05)")

plt.figure(figsize=(14, 8))

# Boxplot mejorado
ax = sns.boxplot(x='furnishingstatus', y='price', data=df, palette='Set2')

# A√±adir puntos de datos
sns.stripplot(x='furnishingstatus', y='price', data=df, size=4, color='black', alpha=0.3)

# A√±adir medias
means = df.groupby('furnishingstatus')['price'].mean()
for i, (category, mean_val) in enumerate(means.items()):
    ax.plot(i, mean_val, marker='D', color='red', markersize=8, label='Media' if i == 0 else "")

plt.title('Distribuci√≥n de Precios por Estado de Amueblado\n(Boxplot con Puntos de Datos)', 
          fontsize=14, fontweight='bold', pad=20)
plt.xlabel('Estado de Amueblado', fontsize=12, fontweight='bold')
plt.ylabel('Precio de la Vivienda (USD)', fontsize=12, fontweight='bold')

# Formato del eje Y
ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))

plt.legend()
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()

print(f"\nüéØ INTERPRETACI√ìN DEL GR√ÅFICO:")
print("  ‚Ä¢ Cada caja muestra la distribuci√≥n de precios para cada categor√≠a")
print("  ‚Ä¢ Los puntos negros son las observaciones individuales")
print("  ‚Ä¢ Los diamantes rojos son las medias de cada grupo")
print("  ‚Ä¢ Es NORMAL ver diferencias entre categor√≠as:")
print("    - 'Furnished' (amueblado) t√≠picamente m√°s caro")
print("    - 'Semi-furnished' (semi-amueblado) precio intermedio") 
print("    - 'Unfurnished' (sin amueblar) generalmente m√°s barato")
print("  ‚Ä¢ Las diferencias reflejan el valor agregado del mobiliario")

# 8. An√°lisis de datos faltantes (MEJORADO)
print("\n" + "="*80)
print("8. AN√ÅLISIS DE DATOS FALTANTES")
print("="*80)

missing_data = df.isnull().sum()
missing_percent = (missing_data / len(df)) * 100

print(f"\nüìä RESUMEN DE DATOS FALTANTES:")
missing_summary = pd.DataFrame({
    'Columna': missing_data.index,
    'Valores Faltantes': missing_data.values,
    'Porcentaje': missing_percent.values
}).round(2)

print(missing_summary.to_string(index=False))

if missing_data.sum() == 0:
    print("\n‚úÖ ¬°EXCELENTE! No hay datos faltantes en el dataset")
    print("  ‚Ä¢ Todas las 545 observaciones est√°n completas")
    print("  ‚Ä¢ No se requiere imputaci√≥n de datos")
else:
    print(f"\n‚ö†Ô∏è  Se encontraron {missing_data.sum()} valores faltantes")

plt.figure(figsize=(12, 6))

# Gr√°fico de barras para datos faltantes
plt.subplot(1, 2, 1)
if missing_data.sum() > 0:
    missing_data[missing_data > 0].plot(kind='bar', color='salmon', alpha=0.8)
    plt.title('Datos Faltantes por Columna', fontsize=12, fontweight='bold')
    plt.xlabel('Columnas del Dataset', fontsize=10, fontweight='bold')
    plt.ylabel('N√∫mero de Valores Faltantes', fontsize=10, fontweight='bold')
else:
    plt.bar(range(len(df.columns)), [0]*len(df.columns), color='lightgreen', alpha=0.8)
    plt.title('Datos Faltantes por Columna\n(Dataset Completo)', fontsize=12, fontweight='bold')
    plt.xlabel('Columnas del Dataset', fontsize=10, fontweight='bold')
    plt.ylabel('N√∫mero de Valores Faltantes', fontsize=10, fontweight='bold')
    plt.xticks(range(len(df.columns)), df.columns, rotation=45)

plt.grid(axis='y', alpha=0.3)

# Matriz de completitud
plt.subplot(1, 2, 2)
msno.matrix(df, ax=plt.gca(), fontsize=8)
plt.title('Matriz de Completitud de Datos\n(Blanco = Faltante, Negro = Presente)', 
          fontsize=12, fontweight='bold')

plt.tight_layout()
plt.show()

print(f"\nüéØ INTERPRETACI√ìN DE LA MATRIZ:")
print("  ‚Ä¢ Cada fila representa una observaci√≥n (vivienda)")
print("  ‚Ä¢ Cada columna representa una variable")
print("  ‚Ä¢ El color negro indica datos presentes")
print("  ‚Ä¢ El color blanco indica datos faltantes")
print(f"  ‚Ä¢ El n√∫mero 545 se repite porque tenemos exactamente 545 observaciones")
print("  ‚Ä¢ Es NORMAL ver 545 en el eje Y (representa el n√∫mero total de filas)")

# 9. Matriz de correlaci√≥n (MEJORADA)
print("\n" + "="*80)
print("9. AN√ÅLISIS DE CORRELACI√ìN ENTRE VARIABLES")
print("="*80)

print("\nüìä F√ìRMULA DE CORRELACI√ìN DE PEARSON:")
print("r = Œ£[(xi - xÃÑ)(yi - »≥)] / ‚àö[Œ£(xi - xÃÑ)¬≤ √ó Œ£(yi - »≥)¬≤]")
print("\nInterpretaci√≥n de valores:")
print("  ‚Ä¢ r = +1: Correlaci√≥n positiva perfecta")
print("  ‚Ä¢ r = +0.7 a +0.9: Correlaci√≥n positiva fuerte")
print("  ‚Ä¢ r = +0.3 a +0.7: Correlaci√≥n positiva moderada")
print("  ‚Ä¢ r = -0.3 a +0.3: Correlaci√≥n d√©bil o nula")
print("  ‚Ä¢ r = -0.3 a -0.7: Correlaci√≥n negativa moderada")
print("  ‚Ä¢ r = -0.7 a -0.9: Correlaci√≥n negativa fuerte")
print("  ‚Ä¢ r = -1: Correlaci√≥n negativa perfecta")

# Calcular matriz de correlaci√≥n
corr_matrix = df[numeric_cols].corr()

print(f"\nüìà MATRIZ DE CORRELACI√ìN:")
print(corr_matrix.round(3).to_string())

# Encontrar correlaciones m√°s fuertes
correlations = []
for i in range(len(corr_matrix.columns)):
    for j in range(i+1, len(corr_matrix.columns)):
        var1 = corr_matrix.columns[i]
        var2 = corr_matrix.columns[j]
        corr_val = corr_matrix.iloc[i, j]
        correlations.append((var1, var2, corr_val))

correlations.sort(key=lambda x: abs(x[2]), reverse=True)

print(f"\nüîç CORRELACIONES M√ÅS FUERTES:")
for var1, var2, corr in correlations[:5]:
    strength = "fuerte" if abs(corr) > 0.7 else "moderada" if abs(corr) > 0.3 else "d√©bil"
    direction = "positiva" if corr > 0 else "negativa"
    print(f"  ‚Ä¢ {var1} ‚Üî {var2}: {corr:.3f} (correlaci√≥n {direction} {strength})")

plt.figure(figsize=(12, 10))

# Crear heatmap mejorado
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
sns.heatmap(corr_matrix, annot=True, cmap='RdBu_r', center=0, 
            square=True, fmt='.3f', cbar_kws={"shrink": .8},
            mask=mask, linewidths=0.5)

plt.title('Matriz de Correlaci√≥n entre Variables Num√©ricas\n(Coeficiente de Correlaci√≥n de Pearson)', 
          fontsize=14, fontweight='bold', pad=20)
plt.xlabel('Variables del Dataset', fontsize=12, fontweight='bold')
plt.ylabel('Variables del Dataset', fontsize=12, fontweight='bold')

# A√±adir texto explicativo
plt.figtext(0.02, 0.02, 'Interpretaci√≥n: Azul = Correlaci√≥n Positiva, Rojo = Correlaci√≥n Negativa\nValores cercanos a ¬±1 indican correlaci√≥n fuerte', 
            fontsize=10, style='italic')

plt.tight_layout()
plt.show()

print(f"\nüéØ INTERPRETACI√ìN DE LA MATRIZ:")
print("  ‚Ä¢ Diagonal principal = 1 (cada variable correlaciona perfectamente consigo misma)")
print("  ‚Ä¢ Colores azules = correlaciones positivas (cuando una sube, la otra tambi√©n)")
print("  ‚Ä¢ Colores rojos = correlaciones negativas (cuando una sube, la otra baja)")
print("  ‚Ä¢ Intensidad del color = fuerza de la correlaci√≥n")
print("  ‚Ä¢ Solo se muestra la mitad inferior para evitar redundancia")

# 10. An√°lisis de Componentes Principales (ACP) - MEJORADO
print("\n" + "="*80)
print("10. AN√ÅLISIS DE COMPONENTES PRINCIPALES (PCA)")
print("="*80)

print("\nüìä FUNDAMENTOS MATEM√ÅTICOS DEL PCA:")
print("Objetivo: Reducir dimensionalidad preservando m√°xima varianza")
print("F√≥rmulas principales:")
print("  ‚Ä¢ Matriz de covarianza: C = (1/(n-1)) √ó X^T √ó X")
print("  ‚Ä¢ Eigenvalores (Œª) y eigenvectors (v): C √ó v = Œª √ó v")
print("  ‚Ä¢ Componentes principales = combinaciones lineales de variables originales")
print("  ‚Ä¢ Varianza explicada por componente i = Œª·µ¢ / Œ£Œª‚±º")

# Preprocesamiento para ACP
le = LabelEncoder()
df_encoded = df.copy()
for col in categorical_cols:
    df_encoded[col] = le.fit_transform(df[col])

print(f"\nüîÑ PREPROCESAMIENTO:")
print("  ‚Ä¢ Variables categ√≥ricas codificadas num√©ricamente (Label Encoding)")
print("  ‚Ä¢ Datos estandarizados (media=0, desviaci√≥n=1)")
print("  ‚Ä¢ F√≥rmula estandarizaci√≥n: z = (x - Œº) / œÉ")

scaler = StandardScaler()
scaled_data = scaler.fit_transform(df_encoded)

pca = PCA()
pca_result = pca.fit_transform(scaled_data)

# An√°lisis de varianza explicada
explained_var = pca.explained_variance_ratio_
cumulative_var = np.cumsum(explained_var)

print(f"\nüìà VARIANZA EXPLICADA POR COMPONENTE:")
for i, (var, cum_var) in enumerate(zip(explained_var, cumulative_var)):
    print(f"  ‚Ä¢ PC{i+1}: {var:.3f} ({var*100:.1f}%) - Acumulada: {cum_var:.3f} ({cum_var*100:.1f}%)")

# Determinar n√∫mero √≥ptimo de componentes
n_components_80 = np.argmax(cumulative_var >= 0.80) + 1
n_components_90 = np.argmax(cumulative_var >= 0.90) + 1

print(f"\nüéØ COMPONENTES RECOMENDADOS:")
print(f"  ‚Ä¢ Para explicar 80% de varianza: {n_components_80} componentes")
print(f"  ‚Ä¢ Para explicar 90% de varianza: {n_components_90} componentes")

# Gr√°ficos de PCA
fig, axes = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle('An√°lisis de Componentes Principales (PCA)', fontsize=16, fontweight='bold')

# Varianza explicada
axes[0,0].bar(range(1, len(explained_var) + 1), explained_var, alpha=0.8, color='skyblue')
axes[0,0].set_title('Varianza Explicada por Componente', fontweight='bold')
axes[0,0].set_xlabel('N√∫mero de Componente')
axes[0,0].set_ylabel('Proporci√≥n de Varianza Explicada')
axes[0,0].grid(axis='y', alpha=0.3)

# Varianza acumulada
axes[0,1].plot(range(1, len(cumulative_var) + 1), cumulative_var, marker='o', linewidth=2, markersize=6)
axes[0,1].axhline(y=0.8, color='r', linestyle='--', label='80% varianza')
axes[0,1].axhline(y=0.9, color='g', linestyle='--', label='90% varianza')
axes[0,1].set_title('Varianza Explicada Acumulada', fontweight='bold')
axes[0,1].set_xlabel('N√∫mero de Componentes')
axes[0,1].set_ylabel('Varianza Explicada Acumulada')
axes[0,1].legend()
axes[0,1].grid(alpha=0.3)

# Contribuci√≥n de variables a PC1 y PC2
loadings = pca.components_.T * np.sqrt(pca.explained_variance_)
axes[1,0].scatter(loadings[:, 0], loadings[:, 1], alpha=0.8, s=100)
for i, col in enumerate(df_encoded.columns):
    axes[1,0].annotate(col, (loadings[i, 0], loadings[i, 1]), 
                      xytext=(5, 5), textcoords='offset points', fontsize=9)
axes[1,0].set_title('Cargas de Variables en PC1 vs PC2', fontweight='bold')
axes[1,0].set_xlabel(f'PC1 ({explained_var[0]*100:.1f}% varianza)')
axes[1,0].set_ylabel(f'PC2 ({explained_var[1]*100:.1f}% varianza)')
axes[1,0].grid(alpha=0.3)
axes[1,0].axhline(y=0, color='k', linestyle='-', alpha=0.3)
axes[1,0].axvline(x=0, color='k', linestyle='-', alpha=0.3)

# Biplot (observaciones en espacio PC1-PC2)
scatter = axes[1,1].scatter(pca_result[:, 0], pca_result[:, 1], 
                           c=df_encoded['price'], cmap='viridis', alpha=0.6)
axes[1,1].set_title('Biplot: Observaciones en Espacio PC1-PC2', fontweight='bold')
axes[1,1].set_xlabel(f'PC1 ({explained_var[0]*100:.1f}% varianza)')
axes[1,1].set_ylabel(f'PC2 ({explained_var[1]*100:.1f}% varianza)')
plt.colorbar(scatter, ax=axes[1,1], label='Precio')
axes[1,1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print(f"\nüéØ INTERPRETACI√ìN DEL PCA:")
print("  ‚Ä¢ PC1 captura la mayor variabilidad en los datos")
print("  ‚Ä¢ Variables con cargas altas en PC1 son las m√°s importantes")
print("  ‚Ä¢ El biplot muestra c√≥mo se distribuyen las observaciones")
print("  ‚Ä¢ Colores en biplot representan precios (amarillo=alto, azul=bajo)")

# 11. Modelo 1: Regresi√≥n log√≠stica con todas las variables (MEJORADO)
print("\n" + "="*80)
print("11. MODELO 1: REGRESI√ìN LOG√çSTICA (TODAS LAS VARIABLES)")
print("="*80)

print("\nüìä FUNDAMENTOS DE REGRESI√ìN LOG√çSTICA:")
print("Funci√≥n log√≠stica: p = 1 / (1 + e^(-z))")
print("donde z = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çôx‚Çô")
print("Para multiclase: Softmax = e^(z·µ¢) / Œ£e^(z‚±º)")

# Preparar datos para el modelo
X = df_encoded.drop('furnishingstatus', axis=1)
y = df_encoded['furnishingstatus']

print(f"\nüîÑ PREPARACI√ìN DE DATOS:")
print(f"  ‚Ä¢ Variables predictoras (X): {X.shape[1]} variables")
print(f"  ‚Ä¢ Variable objetivo (y): furnishingstatus ({y.nunique()} clases)")
print(f"  ‚Ä¢ Distribuci√≥n de clases:")
for class_val, count in y.value_counts().items():
    class_name = ['furnished', 'semi-furnished', 'unfurnished'][class_val]
    print(f"    - Clase {class_val} ({class_name}): {count} observaciones ({count/len(y)*100:.1f}%)")

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y)

print(f"\nüìä DIVISI√ìN DE DATOS:")
print(f"  ‚Ä¢ Entrenamiento: {X_train.shape[0]} observaciones ({X_train.shape[0]/len(df)*100:.1f}%)")
print(f"  ‚Ä¢ Prueba: {X_test.shape[0]} observaciones ({X_test.shape[0]/len(df)*100:.1f}%)")

model1 = LogisticRegression(
    multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42)
model1.fit(X_train, y_train)
y_pred1 = model1.predict(X_test)
y_prob1 = model1.predict_proba(X_test)

print(f"\nüìà RESULTADOS DEL MODELO 1:")
accuracy1 = accuracy_score(y_test, y_pred1)
print(f"  ‚Ä¢ Exactitud (Accuracy): {accuracy1:.4f} ({accuracy1*100:.2f}%)")
print(f"  ‚Ä¢ F√≥rmula Accuracy: (VP + VN) / (VP + VN + FP + FN)")

print(f"\nüìä MATRIZ DE CONFUSI√ìN:")
cm1 = confusion_matrix(y_test, y_pred1)
print(cm1)

print(f"\nüìã REPORTE DETALLADO:")
class_names = ['furnished', 'semi-furnished', 'unfurnished']
report1 = classification_report(y_test, y_pred1, target_names=class_names, output_dict=True)
for class_name in class_names:
    metrics = report1[class_name]
    print(f"\n{class_name.upper()}:")
    print(f"  ‚Ä¢ Precisi√≥n: {metrics['precision']:.3f} (VP/(VP+FP))")
    print(f"  ‚Ä¢ Recall: {metrics['recall']:.3f} (VP/(VP+FN))")
    print(f"  ‚Ä¢ F1-Score: {metrics['f1-score']:.3f} (2√óPrecisi√≥n√óRecall/(Precisi√≥n+Recall))")

# 12. Modelo 2: M√©todo backward (eliminaci√≥n recursiva de variables) - MEJORADO
print("\n" + "="*80)
print("12. MODELO 2: SELECCI√ìN DE VARIABLES (RFE)")
print("="*80)

print("\nüìä RECURSIVE FEATURE ELIMINATION (RFE):")
print("Algoritmo:")
print("  1. Entrenar modelo con todas las variables")
print("  2. Calcular importancia de cada variable")
print("  3. Eliminar la variable menos importante")
print("  4. Repetir hasta alcanzar n√∫mero deseado de variables")

n_features_select = 5
selector = RFE(LogisticRegression(multi_class='multinomial',
               solver='lbfgs', max_iter=1000, random_state=42), 
               n_features_to_select=n_features_select)
selector.fit(X_train, y_train)

selected_features = X.columns[selector.support_]
print(f"\nüéØ VARIABLES SELECCIONADAS ({n_features_select} de {X.shape[1]}):")
for i, feature in enumerate(selected_features, 1):
    print(f"  {i}. {feature}")

print(f"\n‚ùå VARIABLES ELIMINADAS:")
eliminated_features = X.columns[~selector.support_]
for i, feature in enumerate(eliminated_features, 1):
    print(f"  {i}. {feature}")

X_train_selected = selector.transform(X_train)
X_test_selected = selector.transform(X_test)

model2 = LogisticRegression(
    multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42)
model2.fit(X_train_selected, y_train)
y_pred2 = model2.predict(X_test_selected)
y_prob2 = model2.predict_proba(X_test_selected)

print(f"\nüìà RESULTADOS DEL MODELO 2:")
accuracy2 = accuracy_score(y_test, y_pred2)
print(f"  ‚Ä¢ Exactitud (Accuracy): {accuracy2:.4f} ({accuracy2*100:.2f}%)")

print(f"\nüìä MATRIZ DE CONFUSI√ìN:")
cm2 = confusion_matrix(y_test, y_pred2)
print(cm2)

print(f"\nüìã REPORTE DETALLADO:")
report2 = classification_report(y_test, y_pred2, target_names=class_names, output_dict=True)
for class_name in class_names:
    metrics = report2[class_name]
    print(f"\n{class_name.upper()}:")
    print(f"  ‚Ä¢ Precisi√≥n: {metrics['precision']:.3f}")
    print(f"  ‚Ä¢ Recall: {metrics['recall']:.3f}")
    print(f"  ‚Ä¢ F1-Score: {metrics['f1-score']:.3f}")

# 13. Comparaci√≥n de modelos (MEJORADA)
print("\n" + "="*80)
print("13. COMPARACI√ìN DETALLADA DE MODELOS")
print("="*80)

metrics = {
    'Modelo': ['Todas las variables', 'RFE (5 variables)'],
    'Accuracy': [accuracy_score(y_test, y_pred1), accuracy_score(y_test, y_pred2)],
    'Precision': [precision_score(y_test, y_pred1, average='weighted'), 
                  precision_score(y_test, y_pred2, average='weighted')],
    'Recall': [recall_score(y_test, y_pred1, average='weighted'), 
               recall_score(y_test, y_pred2, average='weighted')],
    'F1-Score': [f1_score(y_test, y_pred1, average='weighted'), 
                 f1_score(y_test, y_pred2, average='weighted')],
    'N_Variables': [X.shape[1], n_features_select]
}
metrics_df = pd.DataFrame(metrics)
print("\nüìä TABLA COMPARATIVA:")
print(metrics_df.round(4).to_string(index=False))

# Determinar mejor modelo
if metrics_df.loc[0, 'Accuracy'] > metrics_df.loc[1, 'Accuracy']:
    best_model = "Modelo 1 (Todas las variables)"
    improvement = metrics_df.loc[0, 'Accuracy'] - metrics_df.loc[1, 'Accuracy']
else:
    best_model = "Modelo 2 (RFE)"
    improvement = metrics_df.loc[1, 'Accuracy'] - metrics_df.loc[0, 'Accuracy']

print(f"\nüèÜ MEJOR MODELO: {best_model}")
print(f"  ‚Ä¢ Mejora en accuracy: {improvement:.4f} ({improvement*100:.2f}%)")
print(f"  ‚Ä¢ Consideraciones:")
print(f"    - Modelo 1: Usa toda la informaci√≥n disponible")
print(f"    - Modelo 2: M√°s simple, menos riesgo de sobreajuste")

# 14. Curvas ROC para modelos (multiclase) - MEJORADO
print("\n" + "="*80)
print("14. AN√ÅLISIS ROC (RECEIVER OPERATING CHARACTERISTIC)")
print("="*80)

print("\nüìä FUNDAMENTOS DE CURVAS ROC:")
print("Para clasificaci√≥n multiclase:")
print("  ‚Ä¢ Se crea una curva ROC para cada clase (One-vs-Rest)")
print("  ‚Ä¢ TPR (Sensibilidad) = VP / (VP + FN)")
print("  ‚Ä¢ FPR (1-Especificidad) = FP / (FP + VN)")
print("  ‚Ä¢ AUC = √Årea bajo la curva ROC")
print("  ‚Ä¢ AUC = 0.5: Clasificador aleatorio")
print("  ‚Ä¢ AUC = 1.0: Clasificador perfecto")

y_test_bin = label_binarize(y_test, classes=[0, 1, 2])
n_classes = y_test_bin.shape[1]

# Curva ROC para Modelo 1
fpr1, tpr1, roc_auc1 = dict(), dict(), dict()
for i in range(n_classes):
    fpr1[i], tpr1[i], _ = roc_curve(
        y_test_bin[:, i], y_prob1[:, i])
    roc_auc1[i] = auc(fpr1[i], tpr1[i])

# Curva ROC para Modelo 2
fpr2, tpr2, roc_auc2 = dict(), dict(), dict()
for i in range(n_classes):
    fpr2[i], tpr2[i], _ = roc_curve(
        y_test_bin[:, i], y_prob2[:, i])
    roc_auc2[i] = auc(fpr2[i], tpr2[i])

# Calcular ROC macro-promedio
all_fpr1 = np.unique(np.concatenate([fpr1[i] for i in range(n_classes)]))
mean_tpr1 = np.zeros_like(all_fpr1)
for i in range(n_classes):
    mean_tpr1 += np.interp(all_fpr1, fpr1[i], tpr1[i])
mean_tpr1 /= n_classes
roc_auc1['macro'] = auc(all_fpr1, mean_tpr1)

all_fpr2 = np.unique(np.concatenate([fpr2[i] for i in range(n_classes)]))
mean_tpr2 = np.zeros_like(all_fpr2)
for i in range(n_classes):
    mean_tpr2 += np.interp(all_fpr2, fpr2[i], tpr2[i])
mean_tpr2 /= n_classes
roc_auc2['macro'] = auc(all_fpr2, mean_tpr2)

print(f"\nüìà VALORES AUC POR CLASE:")
print(f"{'Clase':<15} {'Modelo 1':<10} {'Modelo 2':<10}")
print("-" * 35)
for i, class_name in enumerate(class_names):
    print(f"{class_name:<15} {roc_auc1[i]:<10.3f} {roc_auc2[i]:<10.3f}")
print(f"{'Macro-promedio':<15} {roc_auc1['macro']:<10.3f} {roc_auc2['macro']:<10.3f}")

# Plot ROC mejorado
plt.figure(figsize=(15, 10))

# ROC por clase
for i in range(2):  # Solo primeras 2 clases para claridad
    plt.subplot(2, 2, i+1)
    
    plt.plot(fpr1[i], tpr1[i], color='blue', lw=2,
             label=f'Modelo 1 (AUC = {roc_auc1[i]:.3f})')
    plt.plot(fpr2[i], tpr2[i], color='red', lw=2, linestyle='--',
             label=f'Modelo 2 (AUC = {roc_auc2[i]:.3f})')
    plt.plot([0, 1], [0, 1], 'k--', lw=1, alpha=0.5, label='Clasificador Aleatorio')
    
    plt.xlabel('Tasa de Falsos Positivos (FPR)', fontweight='bold')
    plt.ylabel('Tasa de Verdaderos Positivos (TPR)', fontweight='bold')
    plt.title(f'ROC: Clase "{class_names[i]}"', fontweight='bold')
    plt.legend(loc="lower right")
    plt.grid(alpha=0.3)

# ROC macro-promedio
plt.subplot(2, 2, 3)
plt.plot(all_fpr1, mean_tpr1, color='blue', lw=2,
         label=f'Modelo 1 Macro-avg (AUC = {roc_auc1["macro"]:.3f})')
plt.plot(all_fpr2, mean_tpr2, color='red', lw=2, linestyle='--',
         label=f'Modelo 2 Macro-avg (AUC = {roc_auc2["macro"]:.3f})')
plt.plot([0, 1], [0, 1], 'k--', lw=1, alpha=0.5)

plt.xlabel('Tasa de Falsos Positivos (FPR)', fontweight='bold')
plt.ylabel('Tasa de Verdaderos Positivos (TPR)', fontweight='bold')
plt.title('ROC Macro-Promedio', fontweight='bold')
plt.legend(loc="lower right")
plt.grid(alpha=0.3)

# Comparaci√≥n AUC
plt.subplot(2, 2, 4)
classes_extended = class_names + ['Macro-avg']
auc1_values = [roc_auc1[i] for i in range(n_classes)] + [roc_auc1['macro']]
auc2_values = [roc_auc2[i] for i in range(n_classes)] + [roc_auc2['macro']]

x = np.arange(len(classes_extended))
width = 0.35

plt.bar(x - width/2, auc1_values, width, label='Modelo 1', alpha=0.8, color='blue')
plt.bar(x + width/2, auc2_values, width, label='Modelo 2', alpha=0.8, color='red')

plt.xlabel('Clases', fontweight='bold')
plt.ylabel('√Årea Bajo la Curva (AUC)', fontweight='bold')
plt.title('Comparaci√≥n de AUC por Clase', fontweight='bold')
plt.xticks(x, classes_extended, rotation=45)
plt.legend()
plt.grid(axis='y', alpha=0.3)
plt.ylim(0, 1)

plt.tight_layout()
plt.show()

print(f"\nüéØ INTERPRETACI√ìN DE CURVAS ROC:")
print("  ‚Ä¢ Curva m√°s cercana a la esquina superior izquierda = mejor modelo")
print("  ‚Ä¢ AUC > 0.8: Excelente discriminaci√≥n")
print("  ‚Ä¢ AUC 0.7-0.8: Buena discriminaci√≥n")
print("  ‚Ä¢ AUC 0.6-0.7: Discriminaci√≥n aceptable")
print("  ‚Ä¢ AUC < 0.6: Discriminaci√≥n pobre")

# Conclusiones finales
print("\n" + "="*80)
print("CONCLUSIONES FINALES DEL AN√ÅLISIS")
print("="*80)

print(f"\nüéØ RESUMEN EJECUTIVO:")
print(f"  ‚Ä¢ Dataset: {df.shape[0]} viviendas con {df.shape[1]} caracter√≠sticas")
print(f"  ‚Ä¢ Calidad de datos: {'Excelente (sin valores faltantes)' if df.isnull().sum().sum() == 0 else 'Requiere limpieza'}")
print(f"  ‚Ä¢ Distribuci√≥n de precios: {'Sesgada a la derecha (normal en inmuebles)' if df['price'].skew() > 0.5 else 'Aproximadamente sim√©trica'}")
print(f"  ‚Ä¢ Mejor modelo: {best_model}")
print(f"  ‚Ä¢ Accuracy del mejor modelo: {max(accuracy1, accuracy2)*100:.2f}%")

print(f"\nüìä HALLAZGOS CLAVE:")
print("  ‚Ä¢ Los precios NO siguen distribuci√≥n normal (t√≠pico en bienes ra√≠ces)")
print("  ‚Ä¢ Existe correlaci√≥n significativa entre √°rea y precio")
print("  ‚Ä¢ El estado de amueblado influye significativamente en el precio")
print("  ‚Ä¢ La selecci√≥n de variables puede mantener el rendimiento con menos complejidad")

print(f"\nüöÄ RECOMENDACIONES:")
print("  ‚Ä¢ Usar el modelo seleccionado para predicci√≥n de categor√≠as de amueblado")
print("  ‚Ä¢ Considerar transformaciones logar√≠tmicas para normalizar precios")
print("  ‚Ä¢ Explorar modelos no lineales (Random Forest, SVM) para mejor rendimiento")
print("  ‚Ä¢ Recopilar m√°s datos de viviendas de alta gama para balancear el dataset")

print("\n" + "="*80)
print("AN√ÅLISIS COMPLETADO EXITOSAMENTE")
print("="*80)